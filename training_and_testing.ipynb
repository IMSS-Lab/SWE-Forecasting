{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz1hktPhoX2_"
      },
      "outputs": [],
      "source": [
        "batch_size = 4  # Increased batch size for faster training\n",
        "train_dataset = create_dataset(train_indices, batch_size)\n",
        "test_dataset = create_dataset(test_indices, batch_size)\n",
        "\n",
        "# Calculate steps per epoch\n",
        "steps_per_epoch = len(train_indices) // batch_size\n",
        "validation_steps = len(test_indices) // batch_size\n",
        "\n",
        "# Create and compile the model\n",
        "model = SWETransUNet()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model with callbacks for early stopping and reducing learning rate\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=5,  # Reduced to 5 epochs\n",
        "    validation_data=test_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Print final metrics\n",
        "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Training MAE: {history.history['mae'][-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "print(f\"Final Validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Independent Environmental Science Project/Plots/training_history.png')\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Training MAE: {history.history['mae'][-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "print(f\"Final Validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/drive/MyDrive/Independent Environmental Science Project/Model/SWETransUNet_model.keras')\n",
        "\n",
        "# Save the training history\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Independent Environmental Science Project/Model/training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "# Visualize model architecture\n",
        "tf.keras.utils.plot_model(model, to_file='/content/drive/MyDrive/Independent Environmental Science Project/Plots/model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Generate and plot sample predictions\n",
        "def plot_sample_predictions(model, dataset, num_samples=5):\n",
        "    plt.figure(figsize=(20, 4*num_samples))\n",
        "    for i, (X, y_true) in enumerate(dataset.take(num_samples)):\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        for j in range(3):  # Plot each channel\n",
        "            plt.subplot(num_samples, 9, i*9 + j*3 + 1)\n",
        "            plt.imshow(X[0, -1, 0, :, :, j], cmap='viridis')\n",
        "            plt.title(f'Input Ch{j}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 9, i*9 + j*3 + 2)\n",
        "            plt.imshow(y_true[0, :, :, j], cmap='viridis')\n",
        "            plt.title(f'True Ch{j}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 9, i*9 + j*3 + 3)\n",
        "            plt.imshow(y_pred[0, :, :, j], cmap='viridis')\n",
        "            plt.title(f'Pred Ch{j}')\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/Independent Environmental Science Project/Plots/sample_predictions.png')\n",
        "    plt.show()\n",
        "\n",
        "plot_sample_predictions(model, test_dataset)\n",
        "\n",
        "print(\"All visualizations have been saved in the 'Plots' folder.\")"
      ]
    }
  ]
}